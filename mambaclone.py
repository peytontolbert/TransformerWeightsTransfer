import torch
import os
import numpy as np
from models.mamba.generation import LlamaMamba
from models.llama.generation import Llama as OriginalLlama
from typing import List
from sklearn.metrics import accuracy_score, classification_report


def generate_clone(ckpt_dir: str, tokenizer_path: str, **kwargs):
    """
    Generates a single clone of the LLaMA model using Mamba checkpoints.

    Args:
        ckpt_dir (str): The checkpoint directory for LLaMA.
        tokenizer_path (str): The tokenizer path.
        kwargs: Additional arguments like temperature, max_seq_len, etc.
    """
    print("DEBUG: Starting clone generation")
    print(f"\nDEBUG: Initializing clone...")
    clone = LlamaMamba.build(ckpt_dir=ckpt_dir, tokenizer_path=tokenizer_path, **kwargs)
    clone.to("cuda:0")  # Ensure clone is moved to CUDA device
    print(f"DEBUG: Clone created successfully")
    print("DEBUG: Clone generation completed")
    return clone


def generate_target_outputs(ckpt_dir: str, tokenizer_path: str, prompts: List[str], **kwargs) -> List[str]:
    """
    Generates target outputs using the original LLaMA model.

    Args:
        ckpt_dir (str): The checkpoint directory for the original LLaMA model.
        tokenizer_path (str): The tokenizer path.
        prompts (List[str]): Prompts for generating target outputs.

    Returns:
        List[str]: Target outputs generated by the original LLaMA model.
    """
    print("DEBUG: Generating target outputs using the original LLaMA model")
    model = OriginalLlama.build(ckpt_dir=ckpt_dir, tokenizer_path=tokenizer_path, **kwargs)
    results = model.text_completion(prompts, max_gen_len=128, temperature=0.1, top_p=0.9)
    generated_outputs = [result['generation'] for result in results]
    print(f"DEBUG: Target outputs generated: {generated_outputs}")
    return generated_outputs


def evaluate_clone(clone, prompts: List[str], target_outputs: List[str], **kwargs):
    """
    Evaluates the clone on provided prompts and compares its outputs.

    Args:
        clone (LlamaMamba): The clone model to evaluate.
        prompts (List[str]): Prompts for evaluation.
        target_outputs (List[str]): Target outputs for comparison.
    """
    print("DEBUG: Starting evaluation of clone")
    clone_name = "Clone_1"
    print(f"DEBUG: Evaluating {clone_name}...")
    clone_outputs = clone.text_completion(prompts, max_gen_len=128, temperature=0.1, top_p=0.9)
    print(f"DEBUG: {clone_name} outputs: {clone_outputs}")

    similarity_score = compute_similarity(clone_outputs, target_outputs)
    print(f"DEBUG: {clone_name} similarity to target: {similarity_score:.2f}")

    print(f"DEBUG: Clone evaluated with similarity {similarity_score:.2f}")
    return similarity_score


def compute_similarity(outputs: List[str], target_outputs: List[str]) -> float:
    """
    Computes the similarity between model outputs and target outputs.

    Args:
        outputs (List[str]): Model outputs.
        target_outputs (List[str]): Target outputs for comparison.

    Returns:
        float: Similarity score.
    """
    print("DEBUG: Computing similarity between outputs and target outputs")
    similarity = sum([1 for out, target in zip(outputs, target_outputs) if out == target]) / len(target_outputs)
    print(f"DEBUG: Similarity score: {similarity:.2f}")
    return similarity


def train_clone(clone, prompts: List[str], target_outputs: List[str], epochs: int = 5, learning_rate: float = 1e-5, **kwargs):
    """
    Trains the clone to minimize similarity loss against target outputs.

    Args:
        clone (LlamaMamba): The clone model to train.
        prompts (List[str]): Prompts for training.
        target_outputs (List[str]): Target outputs.
        epochs (int): Number of training epochs.
        learning_rate (float): Learning rate for optimizer.
        kwargs: Additional arguments.
    """
    print("DEBUG: Starting training of clone")
    clone.train()
    optimizer = torch.optim.Adam(clone.model.parameters(), lr=learning_rate)  # Accessing model parameters correctly
    for epoch in range(epochs):
        optimizer.zero_grad()
        
        # Tokenize prompts and target outputs
        input_ids = [clone.tokenizer.encode(prompt, bos=True, eos=False) for prompt in prompts]
        target_ids = [clone.tokenizer.encode(target, bos=True, eos=False) for target in target_outputs]

        # Pad sequences and convert to tensors
        inputs = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(ids) for ids in input_ids],
            batch_first=True,
            padding_value=clone.tokenizer.pad_id
        ).to("cuda:0")

        targets = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(ids) for ids in target_ids],
            batch_first=True,
            padding_value=clone.tokenizer.pad_id
        ).to("cuda:0")

        # Forward pass with loss computation
        outputs = clone.model(inputs, target_ids=targets)  # Use 'target_ids' instead of 'labels'
        loss = outputs.loss  # Extract loss from the output

        loss.backward()
        optimizer.step()
        print(f"DEBUG: {clone.__class__.__name__} - Epoch {epoch + 1}/{epochs} - Loss: {loss.item():.4f}")
    print("DEBUG: Training of clone completed")


def main(ckpt_dir, tokenizer_path, prompts, **kwargs):
    # Step 1: Generate Target Outputs from Original LLaMA Model
    print("DEBUG: Step 1 - Generating target outputs")
    target_outputs = generate_target_outputs(ckpt_dir, tokenizer_path, prompts, **kwargs)

    # Step 2: Generate Clone
    print("DEBUG: Step 2 - Generating clone")
    clone = generate_clone(ckpt_dir, tokenizer_path, **kwargs)
    clone.to("cuda:0")  # Ensure clone is moved to CUDA device

    # {{ Start iterative refinement }}
    num_iterations = kwargs.get('num_iterations', 5)
    for iteration in range(num_iterations):
        print(f"\nDEBUG: Iteration {iteration + 1} - Training clone")
        train_clone(clone, prompts, target_outputs, epochs=1, learning_rate=1e-5, **kwargs)

        print(f"DEBUG: Iteration {iteration + 1} - Evaluating clone")
        similarity = evaluate_clone(clone, prompts, target_outputs, **kwargs)

        print(f"DEBUG: Clone similarity after iteration {iteration + 1}: {similarity:.2f}")
    # {{ End iterative refinement }}

    # Step 3: Final Evaluation
    print(f"\nDEBUG: Final similarity achieved: {similarity:.2f}")


if __name__ == "__main__":
    # Updated checkpoint directory to the absolute path where checkpoint files are located
    ckpt_dir: str = "checkpoints/llama-3.2-3b/"
    
    # Updated tokenizer path to match the absolute checkpoint directory
    tokenizer_path: str = "checkpoints/llama-3.2-3b/tokenizer.model"

    prompts = [
        "I believe the meaning of life is",
        "Simply put, the theory of relativity states that ",
    ]

    main(ckpt_dir, tokenizer_path, prompts, max_seq_len=128, max_batch_size=4)